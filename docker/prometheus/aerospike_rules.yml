groups:
- name: aerospike.rules
  rules:
  - alert: NodeDown
    expr: up{job="aerospike"} == 0
    for: 30s
    labels:
      severity: "2"
    annotations:
      description: '{{ $labels.instance }} has been down for more than 30 seconds.'
      summary: Node {{ $labels.instance }} down
  - alert: NodeDown
    expr: aerospike_node_up{job="aerospike"} == 0
    for: 30s
    labels:
      severity: "2"
    annotations:
      description: '{{ $labels.instance }} node is down.'
      summary: Node {{ $labels.instance }} down
  - alert: TransactionQueueTooFull
    expr: aerospike_node_stats_tsvc_queue{job="aerospike"} > 10000
    for: 30s
    labels:
      severity: "1"
    annotations:
      description: 'Transaction queue for node {{ $labels.instance }} is longer than 10000.'
      summary: Transaction queue too full for node {{ $labels.instance }}
  - alert: DiskAlmostFull
    expr: aerospike_namespace_device_used_bytes{job="aerospike"}/aerospike_namespace_device_total_bytes{job="aerospike"} >= 0.90
    for: 30s
    labels:
      severity: "1"
    annotations:
      description: 'Disk space for node {{ $labels.instance }} is 90% full.'
      summary: Disk almost full for node {{ $labels.instance }}
  - alert: MemoryAlmostFull
    expr: aerospike_namespace_memory_used_bytes{job="aerospike"}/aerospike_namespace_memory_size{job="aerospike"} >= 0.90
    for: 30s
    labels:
      severity: "1"
    annotations:
      description: 'Memory for node {{ $labels.instance }} is 90% full.'
      summary: Memory almost full for node {{ $labels.instance }}
  - alert: OutOfContagiousDiskSpace
    expr: aerospike_namespace_device_available_pct{job="aerospike"} <= 10
    for: 30s
    labels:
      severity: "1"
    annotations:
      description: 'Contiguous Disk space available for new writes on namespace {{ $labels.ns }} in node {{ $labels.instance }} is below 10%.'
      summary: Contiguous Disk space running out for node {{ $labels.instance }}/{{ $labels.ns }}
  - alert: DiskUsageOverHighWaterMark
    expr: (100 - aerospike_namespace_device_free_pct{job="aerospike"}) > aerospike_namespace_high_water_disk_pct{job="aerospike"}
    for: 30s
    labels:
      severity: "1"
    annotations:
      description: 'Used Disk space for namespace {{ $labels.ns }} in node {{ $labels.instance }} is above high watermark.'
      summary: Disk usage above high watermark for {{ $labels.instance }}/{{ $labels.ns }}
  - alert: MemoryUsageOverHighWaterMark
    expr: (100 - aerospike_namespace_memory_free_pct{job="aerospike"}) > aerospike_namespace_high_water_memory_pct{job="aerospike"}
    for: 30s
    labels:
      severity: "1"
    annotations:
      description: 'Used Memory space for namespace {{ $labels.ns }} in node {{ $labels.instance }} is above high watermark.'
      summary: Memory usage above high watermark for {{ $labels.instance }}/{{ $labels.ns }}
  - alert: NamespaceDiskStopWrites
    expr: aerospike_namespace_stop_writes{job="aerospike"} == 1
    for: 30s
    labels:
      severity: "1"
    annotations:
      description: 'Used diskspace for namespace {{ $labels.ns }} in node {{ $labels.instance }} is above stop writes limit.'
      summary: Stop Writes for {{ $labels.instance }}/{{ $labels.ns }}
  - alert: NamespaceMemoryStopWrites
    expr: (100 - aerospike_namespace_memory_free_pct{job="aerospike"}) > aerospike_namespace_stop_writes_pct{job="aerospike"}
    for: 30s
    labels:
      severity: "1"
    annotations:
      description: 'Used Memory space for namespace {{ $labels.ns }} in node {{ $labels.instance }} is above stop writes limit.'
      summary: Stop Writes for {{ $labels.instance }}/{{ $labels.ns }}
  - alert: ClockSkewStopWrites
    expr: aerospike_namespace_clock_skew_stop_writes{job="aerospike"} == 1
    for: 30s
    labels:
      severity: "1"
    annotations:
      description: 'Clock has skewed for namespace {{ $labels.ns }} in node {{ $labels.instance }}'
      summary: Writes will be stopped by Aerospike
  - alert: ClusterSize
    expr: aerospike_node_stats_cluster_size{job="aerospike"} < 2 # user has to replace his cluster size with 2 here in the example
    for: 30s
    labels:
      severity: "1"
    annotations:
      description: 'Cluster size mismatch for namespace {{ $labels.ns }} in node {{ $labels.instance }}'
      summary: Some of the node(s) has gone out of the cluster
  - alert: DeadPartitions
    expr: aerospike_namespace_dead_partitions{job="aerospike"} > 0
    for: 30s
    labels:
      severity: "1"
    annotations:
      description: 'Some of the partition(s) for namespace {{ $labels.ns }} in node {{ $labels.instance }} are dead'
      summary: Will require the use of the revive command to make them available again
  - alert: HwmBreached
    expr: aerospike_namespace_hwm_breached{job="aerospike"} == 1
    for: 30s
    labels:
      severity: "1"
    annotations:
      description: 'High water mark has breached for namespace {{ $labels.ns }} in node {{ $labels.instance }}'
      summary: Eviction will start to make the space available
- alert: UnavailablePartitions
    expr: aerospike_namespace_unavailable_partitions{job="aerospike"} > 0
    for: 30s
    labels:
      severity: "1"
    annotations:
      description: 'Some of the partition(s) is not available for namespace {{ $labels.ns }} in node {{ $labels.instance }}'
      summary: Server could not found some partition(s). Check for the network issues and make sure cluster forms properly
- alert: XDRTimelag
    expr: aerospike_node_stats_xdr_timelag{job="aerospike"} > 10 # user can configure the seconds. Refer XDR throttling
    for: 30s
    labels:
      severity: "1"
    annotations:
      description: 'There seems some lag in XDR for namespace {{ $labels.ns }} in node {{ $labels.instance }}'
      summary: Lag can be there in XDR due to network connectivity issues or errors writing at a destination cluster
